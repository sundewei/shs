<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
        "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html>
<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <title>SAP I/O and ETL API (Basic)</title>
    <link href="css/codesite.pack.04102009.css" type="text/css" rel="stylesheet">
</head>

<body class="gc-documentation">

<div id="gc-container">

<div id="gc-topnav">
    <h1>SAP Hadoop I/O and ETL API (Basic)</h1>
    <ul id="docs" class="gc-topnav-tabs">
        <li>
          <a href="codeLab.html" class="selected" title="SAP Hadoop I/O and ETL API (Basic)">[1]</a>
        </li>

        <li>
          <a href="codeLabShs.html" title="SAP Task Force Web App">[2]</a>
        </li>

        <li>
          <a href="codeLabBc.html" title="Biz Case: People You May Know">[3]</a>
        </li>

        <li>
          <a href="examples/logProcessing/log.html" title="Log Processing/Product Trend">[E1]</a>
        </li>
    </ul>
</div>
<!-- end gc-topnav -->

<div class="g-section g-tpl-170">

<div class="g-unit g-first" id="gc-toc">
    <ul>
        <li><h2>Quick Links</h2>
            <ul>
                <li>
                    <a href="#Introduction" title="Introduction">Introduction</a>
                </li>

                <li>
                    <a href="#Prerequisites" title="Prerequisites">Prerequisites</a>
                </li>

                <li>
                    <a href="#Ioapi" title="Hdfs & I/O API">Hdfs & I/O API</a>
                </li>

                <li>
                    <a href="#Etlapi" title="Hive & ETL API">Hive & ETL API</a>
                </li>

                <li>
                    <a href="#Conclusion" title="Conclusion">Conclusion</a>
                </li>

            </ul>
        </li>
    </ul>

</div>

<a name="gc-pagecontent-anchor"></a>

<div class="g-unit" id="gc-pagecontent">

<a name="Introduction"></a>

<h1>Introduction</h1>
<p>
    Before you start with the code lab, please read <a href="http://hadoop.apache.org/" target="_new">here</a> to find out what Hadoop is.
    <br /><br />
    With growing On-Demand business, SAP will need a scale-able batching processing framework in place to handle huge data volumes and complex projects.&nbsp;&nbsp;On the other hand, Internet companies like Facebook, Google, Yahoo, etc., have proved that Hadoop is a perfect solution for that purpose.
    <br /><br />
    Therefore a prototype SAP Hadoop cluster was setup and 2 APIs (I/O and ETL) have been built.  Facilitating the setup of     ETL pipelines over HDFS.<br />
    This code lab will mainly focus on the following: <br />
    <ul class="bulletlist">
        <li> How to use SAP's I/O API</li>
        <li> How to use SQP's ETL API</li>
        <li> How to add dependency among ETL steps</li>
    </ul>
    <br />
    The two I/O and ETL APIs provide a way to upload input files and chain complicated ETL pipeline SQL steps over Hive tables.  This will support potentially huge data volumes.  <br /><br />
    The goal is to provide a SQL interface for developers to use Hive as regular databases and perform powerful data warehousing operations.  <br /> <br />

    To make sure prototype SAP Hadoop cluster is up and running, click on the following links to verify its availability: <br />
            <a href="http://hadoop.pal.sap.corp:50030/jobtracker.jsp" target="_new">Hadoop Job Tracker URL</a> <br />
            <a href="http://hadoop.pal.sap.corp:50070/dfshealth.jsp" target="_new"> HDFS Status URL</a> <br />
            <br />
</p>

<a name="Prerequisites"></a>

<h1>Prerequisites</h1>

<p>
    <ul class="bulletlist">
        <li> Create a directory c:\codelab and c:\data</li>
        <li> Install JDK 1.6 or higher from <a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_new">here</a></li>
        <li> Download to c:\codelab and include Hadoop libraries in your CLASSPATH (download from <a href="hadoop-core-0.20.2-cdh3u2.jar">here</a>)</li>
        <li> Download to c:\codelab and include SAP Hadoop API in your CLASSPATH (download from <a href="../resources/binary/sap_hadoop.jar">here</a>)</li>
        <li> Download to c:\codelab and include 5 Hive-related jar files in your CLASSPATH (download <a href="hive-exec-0.7.1-cdh3u2.jar">hive-exec-0.7.1-cdh3u2</a>, <a href="hive-jdbc-0.7.1-cdh3u2.jar">hive-jdbc-0.7.1-cdh3u2</a>, <a href="hive-metastore-0.7.1-cdh3u2.jar">hive-metastore-0.7.1-cdh3u2</a>, <a href="hive-service-0.7.1-cdh3u2.jar">hive-service-0.7.1-cdh3u2</a>, <a href="libfb303.jar">libfb303</a>)</li>
        <li> Download to c:\codelab and include <a href="log4j-1.2.15.jar">Log4J</a>, <a href="commons-logging-1.0.4.jar">Apache Common Logging API</a> and <a href="commons-io-2.0.1.jar">Apache Common IO API</a> in your CLASSPATH</li>
        <li> (Optional) Request access to SAP Hadoop cluster by sending your employee ID to <a href="mailto:dewei.sun@sap.com">me</a> or you can just use the test account in the code snippets</li>
        <li> The TSV ("Tab-separated values") file used in this code lab can be downloaded <a href="real_category.tsv">here</a>
                (You can save the tsv file to <b>c:\data\</b> which is the folder this code lab uses)</li>
    </ul>
    <br />
    Please contact <a href="mailto:dewei.sun@sap.com">Dewei Sun</a>, if there is any error opening above links.
</p>

<a name="Ioapi"></a>

<h1>HDFS & I/O API</h1>

<p>
    Hadoop Distributed File System (HDFS™) is the primary storage system used by Hadoop applications. Therefore your input files needs to be uploaded to HDFS before beginning your Hadoop job. <br />
    Give the following sample a try:
    <br /><br />
    1. The following will upload the sample file (<a href="real_category.tsv" target="_new">real_category.tsv</a>) to the HDFS.  The file represents real categories from an online encyclopedia and contains 2 fields each line: article_id and category_name and in a later step we will access read these values back using a SQL-like interface.
    <pre>
        <code>
import com.sap.hadoop.conf.ConfigurationManager;
import com.sap.hadoop.etl.ContextFactory;
import com.sap.hadoop.etl.ETLStepContextException;
import com.sap.hadoop.etl.IContext;
import com.sap.hadoop.etl.UploadStep;
import java.io.IOException;
public class Test {
    public static void main(String[] args) throws ETLStepContextException, InterruptedException, IOException {
        <font color="grey">// Get a configuration manager from employee id and your Hadoop password (not your SAP password)</font>
        ConfigurationManager cm = new ConfigurationManager("I123456", "hadoopsap");

        <font color="grey">// Create a context object</font>
        IContext context = ContextFactory.createContext(cm);

        <font color="grey">// Setup an upload job</font>
        UploadStep uploadStep = new UploadStep("RealCategoryUpload");
        uploadStep.setLocalFilename("C:\\data\\real_category.tsv");
        uploadStep.setRemoteFilename(context.getRemoteWorkingFolder() + "real_category.tsv");

        <font color="grey">// Add the step and run it</font>
        context.addStep(uploadStep);
        context.runSteps();
    }
}
        </code>
    </pre>
    <br /><br />

    2. To find out if <a href="real_category.tsv" target="_new">real_category.tsv</a> has been uploaded to your HDFS directory: <br /> <br />
    Open this <a href="ftp://hadoop.pal.sap.corp:22222/user/I123456/" target="_new">URL</a> in your browser and entering your Hadoop username/password or I123456/hadoopsap from the test account.

    <br /><br />

    3. (OPTIONAL) You could upload multiple files by creating more steps to add to the context object: <br /><br />

    Uploading multiple files is as simple as adding more steps to the context. <br />

    <pre>
        <code>
    <font color="grey">// Setup an upload job - 1</font>
    UploadStep upload1 = new UploadStep("file1");
    upload1.setLocalFilename("C:\\data\\file1.xyz");
    upload1.setRemoteFilename(context.getRemoteFolder() + "file1.xyz");

    <font color="grey">// Setup an upload job - 2</font>
    UploadStep upload2 = new UploadStep("file2");
    upload2.setLocalFilename("C:\\data\\file2.xyz");
    upload2.setRemoteFilename(context.getRemoteFolder() + "file2.xyz");

    <font color="grey">// Setup an upload job - 3</font>
    UploadStep upload3 = new UploadStep("file3");
    upload3.setLocalFilename("C:\\data\\file3.xyz");
    upload3.setRemoteFilename(context.getRemoteFolder() + "file3.xyz");

    <font color="grey">// Add the steps and run them</font>
    context.addStep(upload1);
    context.addStep(upload2);
    context.addStep(upload3);
    context.runSteps();
        </code>
    </pre>

    <br /><br />

    (OPTIONAL) The above 3 steps can be replaced by a UploadFolderStep:

    <br /><br />

    <pre>
        <code>
UploadFolderStep uploadFolder = new UploadFolderStep("UploadFolder: temp");
uploadFolder.setLocalFolderName("C:\\temp\\");
uploadFolder.setRemoteFolderName(context.getRemoteFolder() + "temp/");

context.addStep(uploadFolder);
context.runSteps();
        </code>
    </pre>

    <br /><br />

    (OPTIONAL) Similarly, there is a DownloadFileStep and DownloadFolderStep:

    <br /><br />

    <pre>
        <code>

<font color=grey>// Download a file</font>
DownloadFileStep downloadFile = new DownloadFileStep("Download File");

<font color=grey>// Specify the remote and local file names</font>
downloadFile.setRemoteFilename(cm.getRemoteFolder() + "test.txt");
downloadFile.setLocalFilename("c:\\data\\test.txt");

context.addStep(downloadFile);
context.runSteps();
         </code>
    </pre>

    <pre>
        <code>

<font color=grey>// Download a temp folder</font>
DownloadFolderStep downloadFolder = new DownloadFolderStep("DownloadFolder: temp");

<font color=grey>// Specify the remote and local folder names</font>
downloadFolder.setRemoteFolderName(cm.getRemoteFolder() + "temp/");
downloadFolder.setLocalFolderName("c:\\temp");

context.addStep(downloadFolder);
context.runSteps();
        </code>
    </pre>
</p>

<a name="Etlapi"></a>

<h1>Hive & ETL API</h1>

<p>
    Hive is a sub-project developed by Facebook to provide an SQL interface over HDFS and uses MapReduce to perform SQL <br />
    operations.&nbsp;&nbsp;Hive is designed to perform data warehouse operations on extremely huge data volumes over HDFS.<br />
    Prototype SAP ETL API was created based on Hive to support SQL and provide JDBC interface:

    <br /><br />

    1. The following will create a table and load data into the table.  We are introducing the notion of dependency (see the hightlighted line for adding step dependency):

    <br /> <br />

    There are 3 steps in this example, uploading the tsv file (like the first example), creating a table and loading data.  <br />

    The dependency required here is for data loading step <2> wait for table creation step <1>

    <br /><br />

    <pre>
        <code>
import com.sap.hadoop.conf.ConfigurationManager;
import com.sap.hadoop.etl.ContextFactory;
import com.sap.hadoop.etl.ETLStepContextException;
import com.sap.hadoop.etl.IContext;
import com.sap.hadoop.etl.SQLStep;

public static void main(String[] args) throws ETLStepContextException, InterruptedException, IOException {
    ConfigurationManager cm = new ConfigurationManager("I123456", "hadoopsap");
    IContext context = ContextFactory.createContext(cm);

    <font color="grey">
    ///////////////////////////////////////////////////////////////////////
    // <0> Upload the input file "real_category.tsv"
    ///////////////////////////////////////////////////////////////////////
    </font>
    UploadStep uploadStep = new UploadStep("RealCategoryUpload");
    uploadStep.setLocalFilename("C:\\data\\real_category.tsv");
    uploadStep.setRemoteFilename(context.getRemoteWorkingFolder() + "real_category.tsv");
    <font color="grey">
    ///////////////////////////////////////////////////////////////////////
    // <1> Now create "category" table
    ///////////////////////////////////////////////////////////////////////
    </font>
    SQLStep createTableCategory = new SQLStep("CREATE TABLE category");
    createTableCategory.setSql(" CREATE EXTERNAL TABLE IF NOT EXISTS category " +
                              " ( article_wpid INT, category_name STRING ) " +
                              "   ROW FORMAT DELIMITED " +
                              "   FIELDS TERMINATED BY '\t' " +
                              "   LINES TERMINATED BY '\n'" +
                              "   STORED AS TEXTFILE ");
    <font color="grey">
    ///////////////////////////////////////////////////////////////////////
    // <2> Load the TSV to "category" table
    ///////////////////////////////////////////////////////////////////////
    </font>
    SQLStep loadTableCategory = new SQLStep("LOAD TABLE category");
    loadTableCategory.setSql(" LOAD DATA INPATH '" + context.getRemoteWorkingFolder() + "real_category.tsv' " +
                             " OVERWRITE INTO TABLE category");

    context.addStep(uploadStep);                               <font color="grey">// Add <0></font>
    context.addStep(createTableCategory);                      <font color="grey">// Add <1></font>
    <font color=red><strong>
    context.addStep(loadTableCategory, createTableCategory);   <font color="grey">// Add <2> and make it depend on <1></font>
    </strong></font>
    context.runSteps();
}
        </code>
    </pre>

    <br /><br />
    2. The following will perform SQL via JDBC on Hive (Complicated SQL like inner and outer joins are supported and will be included in the advanced code lab): <br /><br />
    This example is to simply print the row count along with the 2 columns in category table using SQL <br />
    <pre>
        <code>
import com.sap.hadoop.conf.ConfigurationManager;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;

public static void main(String[] arg) throws SQLException {
    ConfigurationManager cm = new ConfigurationManager("I123456", "hadoopsap");
    <font color="grey">// Get a JDBC connection to the Hive instance</font>
    Connection conn = cm.getConnection();
    Statement stmt = conn.createStatement();

    <font color="grey">// Get the ResultSet</font>
    ResultSet rs = stmt.executeQuery(" SELECT * FROM category ");
    int resultCount = 1;
    while (rs.next()) {
        System.out.println(resultCount + ", " + rs.getString(1) + ", " + rs.getString(2));
        resultCount++;
    }
    stmt.close();
    conn.close();
}

        </code>
    </pre>

</p>

<a name="Conclusion"></a>

<h1>Conclusion</h1>

<p>
    Next code lab will introduce how to submit an organic MapReduce to SAP Hadoop cluster using SAP Task Force web application.
</p>

</div>
<!-- end gc-pagecontent -->
</div>
<!-- end gooey wrapper -->
</div>
<!-- end codesite content -->
<div id="gc-footer" dir="ltr">
    <div class="text">
        ©2011 SAP
    </div>
</div>
<!-- end gc-footer -->

</body>
</html>

